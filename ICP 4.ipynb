{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNVqfTIK1ccCTqtZe+TQsKl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gpMeSVlb99lq","executionInfo":{"status":"ok","timestamp":1727137113315,"user_tz":300,"elapsed":404667,"user":{"displayName":"Venkata Nitish Bayarapuneni","userId":"02491724517830937665"}},"outputId":"3920efd1-74c3-4836-8e18-1f1c046523e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.4398 - loss: 1.7732 - val_accuracy: 0.9190 - val_loss: 0.2922\n","Epoch 2/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8445 - loss: 0.5362 - val_accuracy: 0.9473 - val_loss: 0.1823\n","Epoch 3/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8997 - loss: 0.3562 - val_accuracy: 0.9599 - val_loss: 0.1387\n","Epoch 4/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9222 - loss: 0.2719 - val_accuracy: 0.9664 - val_loss: 0.1159\n","Epoch 5/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9402 - loss: 0.2140 - val_accuracy: 0.9697 - val_loss: 0.1028\n","Epoch 6/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9480 - loss: 0.1852 - val_accuracy: 0.9727 - val_loss: 0.0957\n","Epoch 7/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9562 - loss: 0.1555 - val_accuracy: 0.9755 - val_loss: 0.0878\n","Epoch 8/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9622 - loss: 0.1316 - val_accuracy: 0.9769 - val_loss: 0.0846\n","Epoch 9/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9663 - loss: 0.1160 - val_accuracy: 0.9772 - val_loss: 0.0835\n","Epoch 10/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9695 - loss: 0.1060 - val_accuracy: 0.9784 - val_loss: 0.0792\n","Epoch 11/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9736 - loss: 0.0919 - val_accuracy: 0.9785 - val_loss: 0.0810\n","Epoch 12/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9767 - loss: 0.0815 - val_accuracy: 0.9784 - val_loss: 0.0793\n","Epoch 13/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9797 - loss: 0.0707 - val_accuracy: 0.9801 - val_loss: 0.0789\n","Epoch 14/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9801 - loss: 0.0701 - val_accuracy: 0.9798 - val_loss: 0.0786\n","Epoch 15/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9818 - loss: 0.0605 - val_accuracy: 0.9788 - val_loss: 0.0806\n","Epoch 16/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9820 - loss: 0.0591 - val_accuracy: 0.9792 - val_loss: 0.0836\n","Epoch 17/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9855 - loss: 0.0524 - val_accuracy: 0.9821 - val_loss: 0.0775\n","Epoch 18/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9838 - loss: 0.0537 - val_accuracy: 0.9810 - val_loss: 0.0780\n","Epoch 19/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9873 - loss: 0.0455 - val_accuracy: 0.9807 - val_loss: 0.0826\n","Epoch 20/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9871 - loss: 0.0436 - val_accuracy: 0.9803 - val_loss: 0.0832\n","Epoch 21/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9868 - loss: 0.0441 - val_accuracy: 0.9817 - val_loss: 0.0777\n","Epoch 22/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9896 - loss: 0.0341 - val_accuracy: 0.9809 - val_loss: 0.0828\n","Epoch 23/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9885 - loss: 0.0383 - val_accuracy: 0.9814 - val_loss: 0.0824\n","Epoch 24/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9905 - loss: 0.0328 - val_accuracy: 0.9811 - val_loss: 0.0794\n","Epoch 25/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9895 - loss: 0.0345 - val_accuracy: 0.9814 - val_loss: 0.0846\n","Epoch 26/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9908 - loss: 0.0306 - val_accuracy: 0.9812 - val_loss: 0.0837\n","Epoch 27/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9892 - loss: 0.0329 - val_accuracy: 0.9811 - val_loss: 0.0852\n","Epoch 28/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9911 - loss: 0.0289 - val_accuracy: 0.9816 - val_loss: 0.0854\n","Epoch 29/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9906 - loss: 0.0300 - val_accuracy: 0.9826 - val_loss: 0.0823\n","Epoch 30/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9917 - loss: 0.0275 - val_accuracy: 0.9827 - val_loss: 0.0849\n","Epoch 31/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9920 - loss: 0.0269 - val_accuracy: 0.9818 - val_loss: 0.0880\n","Epoch 32/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9921 - loss: 0.0269 - val_accuracy: 0.9807 - val_loss: 0.0914\n","Epoch 33/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9925 - loss: 0.0275 - val_accuracy: 0.9821 - val_loss: 0.0894\n","Epoch 34/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9927 - loss: 0.0243 - val_accuracy: 0.9812 - val_loss: 0.0864\n","Epoch 35/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9933 - loss: 0.0220 - val_accuracy: 0.9828 - val_loss: 0.0855\n","Epoch 36/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9929 - loss: 0.0236 - val_accuracy: 0.9835 - val_loss: 0.0807\n","Epoch 37/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9930 - loss: 0.0218 - val_accuracy: 0.9836 - val_loss: 0.0838\n","Epoch 38/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9935 - loss: 0.0217 - val_accuracy: 0.9826 - val_loss: 0.0868\n","Epoch 39/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9935 - loss: 0.0215 - val_accuracy: 0.9826 - val_loss: 0.0861\n","Epoch 40/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9939 - loss: 0.0204 - val_accuracy: 0.9840 - val_loss: 0.0872\n","Epoch 41/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9941 - loss: 0.0194 - val_accuracy: 0.9829 - val_loss: 0.0905\n","Epoch 42/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9947 - loss: 0.0176 - val_accuracy: 0.9822 - val_loss: 0.0958\n","Epoch 43/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9938 - loss: 0.0193 - val_accuracy: 0.9823 - val_loss: 0.0901\n","Epoch 44/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9942 - loss: 0.0192 - val_accuracy: 0.9828 - val_loss: 0.0912\n","Epoch 45/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9936 - loss: 0.0203 - val_accuracy: 0.9827 - val_loss: 0.0888\n","Epoch 46/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9941 - loss: 0.0180 - val_accuracy: 0.9825 - val_loss: 0.0892\n","Epoch 47/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9943 - loss: 0.0185 - val_accuracy: 0.9825 - val_loss: 0.0880\n","Epoch 48/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9941 - loss: 0.0175 - val_accuracy: 0.9827 - val_loss: 0.0876\n","Epoch 49/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9944 - loss: 0.0191 - val_accuracy: 0.9833 - val_loss: 0.0859\n","Epoch 50/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9952 - loss: 0.0150 - val_accuracy: 0.9833 - val_loss: 0.0899\n","Epoch 51/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9945 - loss: 0.0180 - val_accuracy: 0.9843 - val_loss: 0.0879\n","Epoch 52/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9948 - loss: 0.0159 - val_accuracy: 0.9827 - val_loss: 0.0920\n","Epoch 53/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9955 - loss: 0.0153 - val_accuracy: 0.9839 - val_loss: 0.0897\n","Epoch 54/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9953 - loss: 0.0156 - val_accuracy: 0.9837 - val_loss: 0.0884\n","Epoch 55/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9949 - loss: 0.0156 - val_accuracy: 0.9818 - val_loss: 0.0965\n","Epoch 56/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9955 - loss: 0.0143 - val_accuracy: 0.9829 - val_loss: 0.0902\n","Epoch 57/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9948 - loss: 0.0183 - val_accuracy: 0.9838 - val_loss: 0.0898\n","Epoch 58/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9953 - loss: 0.0157 - val_accuracy: 0.9832 - val_loss: 0.0871\n","Epoch 59/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9957 - loss: 0.0139 - val_accuracy: 0.9826 - val_loss: 0.0892\n","Epoch 60/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9962 - loss: 0.0132 - val_accuracy: 0.9822 - val_loss: 0.0913\n","Epoch 61/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9964 - loss: 0.0121 - val_accuracy: 0.9832 - val_loss: 0.0908\n","Epoch 62/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9961 - loss: 0.0129 - val_accuracy: 0.9824 - val_loss: 0.0947\n","Epoch 63/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9953 - loss: 0.0159 - val_accuracy: 0.9823 - val_loss: 0.0967\n","Epoch 64/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9960 - loss: 0.0137 - val_accuracy: 0.9842 - val_loss: 0.0891\n","Epoch 65/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9961 - loss: 0.0128 - val_accuracy: 0.9828 - val_loss: 0.0937\n","Epoch 66/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9964 - loss: 0.0121 - val_accuracy: 0.9835 - val_loss: 0.0904\n","Epoch 67/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9958 - loss: 0.0132 - val_accuracy: 0.9839 - val_loss: 0.0876\n","Epoch 68/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9962 - loss: 0.0125 - val_accuracy: 0.9837 - val_loss: 0.0887\n","Epoch 69/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9963 - loss: 0.0126 - val_accuracy: 0.9833 - val_loss: 0.0915\n","Epoch 70/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9957 - loss: 0.0126 - val_accuracy: 0.9833 - val_loss: 0.0918\n","Epoch 71/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9964 - loss: 0.0119 - val_accuracy: 0.9841 - val_loss: 0.0917\n","Epoch 72/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9971 - loss: 0.0111 - val_accuracy: 0.9829 - val_loss: 0.0889\n","Epoch 73/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9958 - loss: 0.0119 - val_accuracy: 0.9835 - val_loss: 0.0909\n","Epoch 74/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9966 - loss: 0.0115 - val_accuracy: 0.9848 - val_loss: 0.0900\n","Epoch 75/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9963 - loss: 0.0124 - val_accuracy: 0.9844 - val_loss: 0.0864\n","Epoch 76/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9964 - loss: 0.0117 - val_accuracy: 0.9848 - val_loss: 0.0880\n","Epoch 77/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9967 - loss: 0.0106 - val_accuracy: 0.9837 - val_loss: 0.0865\n","Epoch 78/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9967 - loss: 0.0111 - val_accuracy: 0.9837 - val_loss: 0.0920\n","Epoch 79/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9966 - loss: 0.0113 - val_accuracy: 0.9837 - val_loss: 0.0906\n","Epoch 80/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9968 - loss: 0.0105 - val_accuracy: 0.9838 - val_loss: 0.0898\n","Epoch 81/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9964 - loss: 0.0104 - val_accuracy: 0.9841 - val_loss: 0.0883\n","Epoch 82/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9967 - loss: 0.0113 - val_accuracy: 0.9835 - val_loss: 0.0948\n","Epoch 83/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9966 - loss: 0.0130 - val_accuracy: 0.9819 - val_loss: 0.0996\n","Epoch 84/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9970 - loss: 0.0108 - val_accuracy: 0.9839 - val_loss: 0.0934\n","Epoch 85/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9970 - loss: 0.0102 - val_accuracy: 0.9831 - val_loss: 0.0981\n","Epoch 86/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9971 - loss: 0.0088 - val_accuracy: 0.9833 - val_loss: 0.0932\n","Epoch 87/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9969 - loss: 0.0095 - val_accuracy: 0.9845 - val_loss: 0.0907\n","Epoch 88/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9973 - loss: 0.0092 - val_accuracy: 0.9839 - val_loss: 0.0963\n","Epoch 89/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9973 - loss: 0.0086 - val_accuracy: 0.9844 - val_loss: 0.0967\n","Epoch 90/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9974 - loss: 0.0089 - val_accuracy: 0.9845 - val_loss: 0.0899\n","Epoch 91/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9968 - loss: 0.0111 - val_accuracy: 0.9833 - val_loss: 0.0952\n","Epoch 92/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9974 - loss: 0.0093 - val_accuracy: 0.9838 - val_loss: 0.0935\n","Epoch 93/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9972 - loss: 0.0093 - val_accuracy: 0.9845 - val_loss: 0.0872\n","Epoch 94/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9967 - loss: 0.0110 - val_accuracy: 0.9845 - val_loss: 0.0875\n","Epoch 95/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9976 - loss: 0.0078 - val_accuracy: 0.9841 - val_loss: 0.0900\n","Epoch 96/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9974 - loss: 0.0086 - val_accuracy: 0.9841 - val_loss: 0.0916\n","Epoch 97/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9979 - loss: 0.0073 - val_accuracy: 0.9842 - val_loss: 0.0899\n","Epoch 98/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9972 - loss: 0.0100 - val_accuracy: 0.9840 - val_loss: 0.0872\n","Epoch 99/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9977 - loss: 0.0085 - val_accuracy: 0.9843 - val_loss: 0.0933\n","Epoch 100/100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9978 - loss: 0.0080 - val_accuracy: 0.9855 - val_loss: 0.0828\n","\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9827 - loss: 0.0898\n","Test accuracy: 0.9866999983787537\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical\n","\n","# Load the MNIST dataset\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","# Preprocess the data: normalize images and one-hot encode labels\n","x_train = x_train.astype('float32') / 255.0\n","x_test = x_test.astype('float32') / 255.0\n","\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","\n","# Build a Sequential model\n","model = Sequential()\n","\n","# Flatten the input (28x28 images) into a vector of size 784\n","model.add(Flatten(input_shape=(28, 28)))\n","\n","# Add 5 hidden layers with increased neurons and Batch Normalization\n","model.add(Dense(1024, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.3))\n","\n","model.add(Dense(512, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.3))\n","\n","model.add(Dense(256, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.3))\n","\n","model.add(Dense(128, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.3))\n","\n","model.add(Dense(64, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.3))\n","\n","# Add the output layer with 10 neurons (one for each class) and softmax activation\n","model.add(Dense(10, activation='softmax'))\n","\n","# Compile the model using the 'adam' optimizer with a lower learning rate\n","optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n","model.compile(optimizer=optimizer,\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Train the model with increased epochs\n","model.fit(x_train, y_train, epochs=100, batch_size=64, validation_split=0.2)\n","\n","# Evaluate the model on the test data\n","test_loss, test_acc = model.evaluate(x_test, y_test)\n","print(f'Test accuracy: {test_acc}')"]}]}